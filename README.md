# ü§ñ Smart-ChatBot ü§ñ

**Smart-ChatBot** es un motor de chatbot dise√±ado para ser vers√°til y f√°cil de configurar. Permite alternar entre modelos locales (v√≠a Ollama) y modelos en la nube (v√≠a OpenAI) con solo cambiar un archivo de configuraci√≥n, manteniendo una interfaz consistente para el usuario.

## Requisitos previos

Antes de empezar, aseg√∫rate de tener instalado:

- [uv](https://github.com/astral-sh/uv): Gestor de paquetes y entornos de Python ultra r√°pido.
- Ollama: si planeas usar modelos locales.

---

## Instalaci√≥n y configuraci√≥n

Sigue estos pasos para poner en marcha tu instancia de SmartBot:

1. Clonar el repositorio e instalar dependencias
Desde la carpeta ra√≠z del proyecto, ejecuta el siguiente comando para sincronizar el entorno y las dependencias:

```cmd
uv sync
```

2. Configuraci√≥n del entorno (`.env`)
Crea un archivo llamado `.env` en la ra√≠z del proyecto y a√±ade tu clave de API si vas a utilizar OpenAI:

```cmd
api_key_openai=<API_KEY>
```

3. Configuraci√≥n del bot (`config.yaml`)
El archivo `config.yaml` es el coraz√≥n de la configuraci√≥n. Aqu√≠ puedes definir qu√© cerebro usar√° tu bot:

```YAML
bot_name: SmartBot

llm:
  provider: ollama        # Opciones: 'ollama' o 'openai'
  base_url: http://localhost:11434  # Requerido solo para Ollama
  model_name: llama3.2:1b # El modelo espec√≠fico a ejecutar
  temperature: 0.7        # Creatividad del modelo (0.0 a 1.0)
```

---

## Uso del proyecto

Para iniciar el chatbot, una vez configurado el entorno, simplemente ejecuta:

```cmd
uv run python main.py
```

## Documentaci√≥n de la configuraci√≥n (API Interna)

El sistema utiliza una l√≥gica de discernimiento basada en el campo `provider`. A continuaci√≥n se detallan los par√°metros:

| **Par√°metro** | **Tipo** | **Descripci√≥n**                                                                                                   |
| ------------- | -------- | ----------------------------------------------------------------------------------------------------------------- |
| `bot_name`    | String   | El nombre con el que el bot se identificar√° en la consola.                                                        |
| `provider`    | String   | **El selector clave.** Determina la clase encargada de la comunicaci√≥n (`ollama` para local, `openai` para nube). |
| `base_url`    | URL      | Endpoint del servidor local. Solo es procesado si el provider es `ollama`.                                        |
| `model_name`  | String   | Identificador del modelo (ej: `gpt-4o`, `llama3.2:1b`, `mistral`).                                                |
| `temperature` | Float    | Controla la aleatoriedad. Valores bajos son m√°s precisos; altos son m√°s creativos.                                |
| `top_p`       | Float    | Define el umbral de probabilidad acumulada para la selecci√≥n de tokens.                                           |

---

## Ejemplo de uso

Si configuras el `provider` como `ollama` y el `model_name` como `llama3.2:1b` al ejecutar el programa ver√°s algo como esto:

```cmd
SmartBot CLI ‚Äî type /exit to quit.
>
```

¬°Ya est√°s preparado para hablar con el smart-chatbot!

```cmd
SmartBot CLI ‚Äî type /exit to quit.
> ¬°Hola smartbot! Cu√©ntame algo sobre ti: qu√© modelo usas, cu√°l es tu proveedor, qu√© se te puede preguntar...
¬°Hola! Me alegra que me hayas preguntado eso.

Soy un modelo de lenguaje grande (LLM) llamado Llama. Llama significa "Meta-Llama". Fui desarrollado por Meta, y he sido entrenado en una gran cantidad de texto para que pueda entender y generar lenguaje humano.

Me puedo preguntar:

* ¬øQu√© te gustar√≠a saber sobre m√≠?
* ¬øCu√°l es tu pregunta favorita sobre m√≠ o otros t√≥picos?
* ¬øHay algo espec√≠fico que me desees saber o discutir?

Puedes intentarlo con una de las siguientes preguntas, o tienes alguna otra idea en mente. Estoy aqu√≠ para ayudarte!
>
```
