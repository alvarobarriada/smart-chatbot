import os

import requests
from base import BaseProvider
from smartbot.utils.config_loader import 


class OllamaProvider(BaseProvider):
    def __init__(self, config: dict[str, str] | None) -> None:
        self.model = os.getenv("OLLAMA_MODEL")
        self.base_url = os.getenv("OLLAMA_URL")

    def generate_response(self, prompt: str, history: list[dict[str, str]]) -> str:
        """
        Generates a response from the LLM by processing the current prompt and chat history.

        :param prompt: The current message from the user.
        :type prompt: str
        :param history: A list of prior chat messages. Each dictionary must contain 'role'
        (e.g., 'user', 'assistant') and 'content' keys.
        :type history: list[dict[str, str]]
        :return: The text content of the response generated by the model.
        :rtype: str
        """
        messages_history = history + [{"role":"user", "content":prompt}]

        response_LLM = requests.post(
            f"{self.base_url}/api/chat",
            json={
                "model": self.model,
                "messages": messages_history,
                "stream": False
            }
        )
        return response_LLM.json()["message"]["content"]

    def validate_config(self) -> bool:
        """
        Validates the connectivity and configuration of the LLM provider.

        :return: True if the service returns a 200 status code, False otherwise.
        :rtype: bool
        """
        try:
            test_request = requests.get(f"{self.base_url}/api/tags")
            return test_request.status_code == 200
        except requests.RequestException:
            return False
