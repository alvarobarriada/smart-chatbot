import requests
from .models import OllamaConfig
from smartbot.memory.models import Message





class OllamaProvider:
    def __init__(self, config: OllamaConfig) -> None:
        self.config = config

    def generate_response(self, prompt: str, history: list[Message]) -> Message:
        """
        Generates a response from the LLM by processing the current prompt and chat history.

        :param prompt: The current message from the user.
        :type prompt: str
        :param history: A list of prior chat messages. Each dictionary must contain 'role'
        (e.g., 'user', 'assistant') and 'content' keys.
        :type history: list[dict[str, str]]
        :return: The text content of the response generated by the model.
        :rtype: str
        """
        messages_history = history + [Message("user",prompt)]

        response_llm = requests.post(
            f"{self.config.base_url}/api/chat",
            json={
                "model": self.config.model_name,
                "messages": [message.to_dict() for message in messages_history],
                "stream": False,
                "options": {
                    "temperature": self.config.temperature,
                    "top_p ": self.config.top_p,
                }
            }
        )
        assistant_message = Message(role="assistant",content=response_llm.json()["message"]["content"])

        return assistant_message


    def validate_config(self) -> bool:
        """
        Validates the connectivity and configuration of the LLM provider.

        :return: True if the service returns a 200 status code, False otherwise.
        :rtype: bool
        """
        try:
            test_request = requests.get(f"{self.config.base_url}/api/tags")
            return test_request.status_code == 200
        except requests.RequestException:
            return False
