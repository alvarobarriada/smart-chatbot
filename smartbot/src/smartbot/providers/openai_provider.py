from typing import List, cast

from openai import APIConnectionError, AuthenticationError, OpenAI
from openai.types.chat import ChatCompletionMessageParam

from smartbot.memory.models import Message

from .models import OpenAIConfig


class OpenaiProvider:
    def __init__(self, config: OpenAIConfig) -> None:
        self.config = config
        self.client = OpenAI(api_key=config.api_key.get_secret_value())

    def generate_response(self, prompt: str, history: list[Message]) -> Message|None:
        """
        Generates a response from the LLM by processing the current prompt and chat history.

        :param prompt: The current message from the user.
        :type prompt: str
        :param history: A list of prior chat messages. Each dictionary must contain 'role'
        (e.g., 'user', 'assistant') and 'content' keys.
        :type history: list[dict[str, str]]
        :return: The text content of the response generated by the model.
        :rtype: str
        """

        messages_history = [*history, Message("user", prompt)]

        response_llm = self.client.chat.completions.create(
            model = self.config.model_name,
            messages=cast(List[ChatCompletionMessageParam],[message.to_dict() for message in messages_history]),
            temperature=self.config.temperature,
            top_p=self.config.top_p,
        )
        text_response_llm = response_llm.choices[0].message.content
        if (text_response_llm is None):
            raise ValueError("No response from LLM")
        assistant_message = Message(role="assistant",content=text_response_llm)

        return assistant_message

    def validate_config(self) -> bool:
        """
        Validates the connectivity and configuration of the LLM provider.

        :return: True if the service returns a 200 status code, False otherwise.
        :rtype: bool
        """
        try:
            self.client.models.list()
            return True
        except (AuthenticationError, APIConnectionError):
            return False
